from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
Important Terms
Let's quickly go over some important terms:

Term	Definition
RDD	Resilient Distributed Dataset
Transformation	Spark operation that produces an RDD
Creating an RDD
There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.

Method	Result
sc.parallelize(array)	Create RDD of elements of array (or list)
sc.textFile(path/to/file)	Create RDD of lines from file
data = [1,2,3,4,5]
rdd = sc.parallelize(data)
rdd
ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274
rdd.collect()
[1, 2, 3, 4, 5]
reduce
rdd.reduce(lambda a,b: a+b)
15
PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.

%%writefile example.txt
first line
second line
third line
fourth line
Overwriting example.txt
rdd1 = sc.textFile('example.txt')
rdd1
example.txt MapPartitionsRDD[3] at textFile at <unknown>:0
rdd1.collect()
['first line', 'second line', 'third line', 'fourth line']
rdd1.getNumPartitions()
2
Count
rdd1.count()
4
First
rdd1.first()
'first line'
Filter
Transformation / Narrow: Return a new RDD containing only the elements that satisfy a predicate



rdd2 = rdd1.filter(lambda line:'second' in line)
rdd2
PythonRDD[7] at RDD at PythonRDD.scala:53
rdd2.collect()
['second line']
rdd3 = rdd1.filter(lambda line: 'third' in line)
rdd3.collect()
['third line']
RDD Transformations
We can use transformations to create a set of instructions we want to preform on the RDD (before we call an action and actually execute them).

Transformations are the process which are used to create a new RDD. It follows the principle of Lazy Evaluations (the execution will not start until an action is triggered).

Transformation Example	Result
filter(lambda x: x % 2 == 0)	Discard non-even elements
map(lambda x: x * 2)	Multiply each RDD element by 2
map(lambda x: x.split())	Split each string into words
flatMap(lambda x: x.split())	Split each string into words and flatten sequence
distinct()	Remove duplicates in RDD
RDD Actions
Once you have your 'recipe' of transformations ready, what you will do next is execute them by calling an action.

Actions are the processes which are applied on an RDD to initiate Apache Spark to apply calculation and pass the result back to driver.

Here are some common actions:

Action	Result
collect()	Convert RDD to in-memory list
take(3)	First 3 elements of RDD
sum()	Find element sum (assumes numeric elements)
mean()	Find element mean (assumes numeric elements)
stdev()	Find element deviation (assumes numeric elements)
Collect
Action / To Driver: Return all items in the RDD to the driver in a single list



rdd1.collect()
['first line', 'second line', 'third line', 'fourth line']
Transformation
In Spark, the core data structures are immutable meaning they cannot be changed once created. This might seem like a strange concept at first, if you cannot change it, how are you supposed to use it? In order to “change” a DataFrame you will have to instruct Spark how you would like to modify the DataFrame you have into the one that you want. These instructions are called transformations. Transformations are the core of how you will be expressing your business logic using Spark. There are two types of transformations, those that specify narrow dependencies and those that specify wide dependencies. https://databricks.com/glossary/what-are-transformations

Narrow transformation — specify narrow dependencies Narrow transformation are those where each input partition will contribute to only one output partition. image.png

Wide transformation — specify wide dependencies. Wide transformation will have input partitions contributing to many output partitions. You will often hear this referred to as a shuffle where Spark will exchange partitions across the cluster. image-2.png

Map
Transformation / Narrow: Return a new RDD by applying a function to each element of this RDD



rdd = sc.parallelize(list(range(8)))
print('rdd elements:',rdd.collect())
rdd elements: [0, 1, 2, 3, 4, 5, 6, 7]
rdd_squared = rdd.map(lambda x: x ** 2).collect() # Square each element
print('rdd elements squared: ', rdd_squared)
rdd elements squared:  [0, 1, 4, 9, 16, 25, 36, 49]
rdd.filter(lambda x : x%2==0).collect()
[0, 2, 4, 6]
Map vs flatMap
FlatMap
Transformation / Narrow: Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results



rdd1.map(lambda line: line.split()).collect() #return list of lists
[['first', 'line'], ['second', 'line'], ['third', 'line'], ['fourth', 'line']]
# Collect everything as a single flat map
flat_rdd = rdd1.flatMap(lambda line: line.split())
flat_rdd.collect()
['first', 'line', 'second', 'line', 'third', 'line', 'fourth', 'line']
GroupBy
Transformation / Wide: Group the data in the original RDD. Create pairs where the key is the output of a user function, and the value is all items for which the function yields this key.



data = ['John','Fred','Anna','James','Frank']
rdd = sc.parallelize(data)
rdd2 = rdd.groupBy(lambda w : w[0])
rdd2.collect()
[('J', <pyspark.resultiterable.ResultIterable at 0x1e480e1fb50>),
 ('F', <pyspark.resultiterable.ResultIterable at 0x1e480e29310>),
 ('A', <pyspark.resultiterable.ResultIterable at 0x1e480e292b0>)]
rdd2_lst = rdd2.collect()
[(k,list(v)) for (k,v) in rdd2_lst]
[('J', ['John', 'James']), ('F', ['Fred', 'Frank']), ('A', ['Anna'])]
Join
Transformation / Wide: Return a new RDD containing all pairs of elements having the same key in the original RDDs



rdd1 = sc.parallelize([("a", 1), ("b", 2)])
rdd2 = sc.parallelize([("a", 3), ("a", 4), ("b", 5)])
rdd1.join(rdd2).collect()
[('a', (1, 3)), ('a', (1, 4)), ('b', (2, 5))]
Distinct
Transformation / Wide: Return a new RDD containing distinct items from the original RDD (omitting all duplicates)



rdd_dis = sc.parallelize([1,2,3,3,4,5,10,5,5,5,2,2,2])
rdd_dis.distinct().collect()
[4, 1, 5, 2, 10, 3]
Exercise
Max, Min, Sum, Mean, Variance, Stdev
Action / To Driver: Compute the respective function (maximum value, minimum value, sum, mean, variance, or standard deviation) from a numeric RDD



rdd_num = sc.parallelize(list(range(8)))
rdd_num.collect()
[0, 1, 2, 3, 4, 5, 6, 7]
# Using actions
print('Max: ',rdd_num.max())
print('Min: ',rdd_num.min())
print('Sum: ',rdd_num.sum())
print('Mean: ',rdd_num.mean())
print('Variance: ',rdd_num.variance())
print('Stdev: ',rdd_num.stdev())
Max:  7
Min:  0
Sum:  28
Mean:  3.5
Variance:  5.25
Stdev:  2.29128784747792